[2024-03-29 07:24:48,577] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-29 07:24:48,702] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-29 07:24:48,708] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 07:24:48,880] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-29 07:24:48,884] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 07:24:48,999] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-29 07:24:49,069] {standard_task_runner.py:52} INFO - Started process 609 to run task
[2024-03-29 07:24:49,159] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '1469', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpyz8fklgw', '--error-file', '/tmp/tmpa62ngpmn']
[2024-03-29 07:24:49,250] {standard_task_runner.py:77} INFO - Job 1469: Subtask local_to_gcs_task
[2024-03-29 07:24:49,783] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host b8c510ab3cfc
[2024-03-29 07:24:50,131] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-29 07:24:50,265] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-29 07:24:51,835] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2022-03.parquet uploaded to yellowtaxi_tripdata/2022-03.parquet.
[2024-03-29 07:24:51,835] {python.py:175} INFO - Done. Returned value was: None
[2024-03-29 07:24:52,090] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240329T072448, end_date=20240329T072452
[2024-03-29 07:24:52,237] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-29 07:24:52,329] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-29 07:31:04,954] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-29 07:31:05,021] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-29 07:31:05,029] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 07:31:05,030] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-29 07:31:05,030] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 07:31:05,114] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-29 07:31:05,143] {standard_task_runner.py:52} INFO - Started process 1102 to run task
[2024-03-29 07:31:05,174] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '1529', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp1cebzf05', '--error-file', '/tmp/tmp7bmttq8g']
[2024-03-29 07:31:05,251] {standard_task_runner.py:77} INFO - Job 1529: Subtask local_to_gcs_task
[2024-03-29 07:31:05,785] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host b8c510ab3cfc
[2024-03-29 07:31:06,125] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-29 07:31:06,201] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-29 07:31:07,397] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2022-03.parquet uploaded to yellowtaxi_tripdata/2022-03.parquet.
[2024-03-29 07:31:07,398] {python.py:175} INFO - Done. Returned value was: None
[2024-03-29 07:31:07,426] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240329T073104, end_date=20240329T073107
[2024-03-29 07:31:07,621] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-29 07:31:07,734] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-29 07:37:50,200] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-29 07:37:50,289] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-29 07:37:50,297] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 07:37:50,298] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-29 07:37:50,299] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 07:37:50,352] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-29 07:37:50,367] {standard_task_runner.py:52} INFO - Started process 1633 to run task
[2024-03-29 07:37:50,409] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '1592', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpn2wwaqa5', '--error-file', '/tmp/tmpo42g0_h8']
[2024-03-29 07:37:50,454] {standard_task_runner.py:77} INFO - Job 1592: Subtask local_to_gcs_task
[2024-03-29 07:37:50,636] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host b8c510ab3cfc
[2024-03-29 07:37:50,801] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-29 07:37:50,917] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-29 07:37:52,294] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2022-03.parquet uploaded to yellowtaxi_tripdata/2022-03.parquet.
[2024-03-29 07:37:52,294] {python.py:175} INFO - Done. Returned value was: None
[2024-03-29 07:37:52,340] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240329T073750, end_date=20240329T073752
[2024-03-29 07:37:52,544] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-29 07:37:52,713] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-29 08:08:46,268] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-29 08:08:46,313] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-29 08:08:46,313] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 08:08:46,313] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-29 08:08:46,313] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 08:08:46,612] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-29 08:08:46,863] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '1653', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpyigg642y', '--error-file', '/tmp/tmp2067i9o0']
[2024-03-29 08:08:46,873] {standard_task_runner.py:77} INFO - Job 1653: Subtask local_to_gcs_task
[2024-03-29 08:08:46,811] {standard_task_runner.py:52} INFO - Started process 308 to run task
[2024-03-29 08:08:47,565] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host b5e5a5778b83
[2024-03-29 08:08:47,792] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-29 08:08:47,944] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-29 08:08:49,689] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2594, in upload_from_file
    retry=retry,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2412, in _do_upload
    retry=retry,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2242, in _do_resumable_upload
    response = upload.transmit_next_chunk(transport, timeout=timeout)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py", line 516, in transmit_next_chunk
    retriable_request, self._get_status_code, self._retry_strategy
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/_request_helpers.py", line 170, in wait_and_retry
    raise error
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/_request_helpers.py", line 147, in wait_and_retry
    response = func()
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py", line 511, in retriable_request
    self._process_response(result, len(payload))
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/_upload.py", line 675, in _process_response
    callback=self._make_invalid,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/_helpers.py", line 104, in require_status_code
    *status_codes
google.resumable_media.common.InvalidResponse: ('Request failed with status code', 429, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PERMANENT_REDIRECT: 308>)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 43, in upload_to_gcs
    blob.upload_from_filename(source_file_name)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2735, in upload_from_filename
    retry=retry,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2598, in upload_from_file
    _raise_from_invalid_response(exc)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 4466, in _raise_from_invalid_response
    raise exceptions.from_http_status(response.status_code, message, response=response)
google.api_core.exceptions.TooManyRequests: 429 PUT https://storage.googleapis.com/upload/storage/v1/b/test_bucket_415106/o?uploadType=resumable&upload_id=ABPtcPpKoZcKpRqK06vqvSVqYdJHRWFhskqDuENlDfEqmGGljEQt2ntscurRyg3AqHjEVSg7EwRWSSN49a6eIkiPqMkeiO2MBiMOYoHG6-ua0j1PHVc: {
  "error": {
    "code": 429,
    "message": "The object test_bucket_415106/yellowtaxi_tripdata/{TABLE_NAME_TEMPLATE}.parquet exceeded the rate limit for object mutation operations (create, update, and delete). Please reduce your request rate. See https://cloud.google.com/storage/docs/gcs429.",
    "errors": [
      {
        "message": "The object test_bucket_415106/yellowtaxi_tripdata/{TABLE_NAME_TEMPLATE}.parquet exceeded the rate limit for object mutation operations (create, update, and delete). Please reduce your request rate. See https://cloud.google.com/storage/docs/gcs429.",
        "domain": "usageLimits",
        "reason": "rateLimitExceeded"
      }
    ]
  }
}
: ('Request failed with status code', 429, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PERMANENT_REDIRECT: 308>)
[2024-03-29 08:08:50,367] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240329T080846, end_date=20240329T080850
[2024-03-29 08:08:50,497] {standard_task_runner.py:92} ERROR - Failed to execute job 1653 for task local_to_gcs_task
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2594, in upload_from_file
    retry=retry,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2412, in _do_upload
    retry=retry,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2242, in _do_resumable_upload
    response = upload.transmit_next_chunk(transport, timeout=timeout)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py", line 516, in transmit_next_chunk
    retriable_request, self._get_status_code, self._retry_strategy
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/_request_helpers.py", line 170, in wait_and_retry
    raise error
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/_request_helpers.py", line 147, in wait_and_retry
    response = func()
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py", line 511, in retriable_request
    self._process_response(result, len(payload))
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/_upload.py", line 675, in _process_response
    callback=self._make_invalid,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/_helpers.py", line 104, in require_status_code
    *status_codes
google.resumable_media.common.InvalidResponse: ('Request failed with status code', 429, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PERMANENT_REDIRECT: 308>)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 43, in upload_to_gcs
    blob.upload_from_filename(source_file_name)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2735, in upload_from_filename
    retry=retry,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2598, in upload_from_file
    _raise_from_invalid_response(exc)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 4466, in _raise_from_invalid_response
    raise exceptions.from_http_status(response.status_code, message, response=response)
google.api_core.exceptions.TooManyRequests: 429 PUT https://storage.googleapis.com/upload/storage/v1/b/test_bucket_415106/o?uploadType=resumable&upload_id=ABPtcPpKoZcKpRqK06vqvSVqYdJHRWFhskqDuENlDfEqmGGljEQt2ntscurRyg3AqHjEVSg7EwRWSSN49a6eIkiPqMkeiO2MBiMOYoHG6-ua0j1PHVc: {
  "error": {
    "code": 429,
    "message": "The object test_bucket_415106/yellowtaxi_tripdata/{TABLE_NAME_TEMPLATE}.parquet exceeded the rate limit for object mutation operations (create, update, and delete). Please reduce your request rate. See https://cloud.google.com/storage/docs/gcs429.",
    "errors": [
      {
        "message": "The object test_bucket_415106/yellowtaxi_tripdata/{TABLE_NAME_TEMPLATE}.parquet exceeded the rate limit for object mutation operations (create, update, and delete). Please reduce your request rate. See https://cloud.google.com/storage/docs/gcs429.",
        "domain": "usageLimits",
        "reason": "rateLimitExceeded"
      }
    ]
  }
}
: ('Request failed with status code', 429, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PERMANENT_REDIRECT: 308>)
[2024-03-29 08:08:50,614] {local_task_job.py:154} INFO - Task exited with return code 1
[2024-03-29 08:08:50,980] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-03-29 08:11:20,096] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-29 08:11:20,138] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-29 08:11:20,138] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 08:11:20,139] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-29 08:11:20,139] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 08:11:20,237] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-29 08:11:20,338] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '1702', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpafz_gdg8', '--error-file', '/tmp/tmp6ulxe9y2']
[2024-03-29 08:11:20,363] {standard_task_runner.py:77} INFO - Job 1702: Subtask local_to_gcs_task
[2024-03-29 08:11:20,246] {standard_task_runner.py:52} INFO - Started process 603 to run task
[2024-03-29 08:11:20,703] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host b5e5a5778b83
[2024-03-29 08:11:21,014] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-29 08:11:21,087] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-29 08:11:22,666] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2022-03.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2022_03.parquet.
[2024-03-29 08:11:22,667] {python.py:175} INFO - Done. Returned value was: None
[2024-03-29 08:11:23,066] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240329T081120, end_date=20240329T081123
[2024-03-29 08:11:23,276] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-29 08:11:23,377] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-30 03:30:23,279] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 03:30:23,379] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 03:30:23,382] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 03:30:23,382] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 03:30:23,382] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 03:30:23,488] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-30 03:30:23,525] {standard_task_runner.py:52} INFO - Started process 546 to run task
[2024-03-30 03:30:23,582] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '1843', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpvh42k3w5', '--error-file', '/tmp/tmprw6ov5is']
[2024-03-30 03:30:23,655] {standard_task_runner.py:77} INFO - Job 1843: Subtask local_to_gcs_task
[2024-03-30 03:30:24,125] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 03:30:24,338] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 03:30:24,486] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-30 03:30:26,292] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2022-03.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2022_03.parquet.
[2024-03-30 03:30:26,295] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 03:30:26,335] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240330T033023, end_date=20240330T033026
[2024-03-30 03:30:26,435] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 03:30:26,658] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-30 03:42:18,292] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 03:42:18,415] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 03:42:18,415] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 03:42:18,419] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 03:42:18,423] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 03:42:18,462] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-30 03:42:18,489] {standard_task_runner.py:52} INFO - Started process 1387 to run task
[2024-03-30 03:42:18,527] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '1924', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpc5c95fit', '--error-file', '/tmp/tmpp4_8xkgv']
[2024-03-30 03:42:18,552] {standard_task_runner.py:77} INFO - Job 1924: Subtask local_to_gcs_task
[2024-03-30 03:42:19,073] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 03:42:19,453] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 03:42:19,663] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-30 03:42:20,823] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2022-03.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2022_03.parquet.
[2024-03-30 03:42:20,824] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 03:42:21,769] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240330T034218, end_date=20240330T034221
[2024-03-30 03:42:21,935] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 03:42:22,116] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-30 04:18:30,365] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 04:18:30,427] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 04:18:30,428] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:18:30,429] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 04:18:30,429] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:18:31,078] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-30 04:18:31,220] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '2109', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpoy70fton', '--error-file', '/tmp/tmp_ei4e9w6']
[2024-03-30 04:18:31,240] {standard_task_runner.py:77} INFO - Job 2109: Subtask local_to_gcs_task
[2024-03-30 04:18:31,156] {standard_task_runner.py:52} INFO - Started process 3637 to run task
[2024-03-30 04:18:31,707] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 04:18:32,033] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 04:18:32,155] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-30 04:18:33,550] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2022-03.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2022_03.parquet.
[2024-03-30 04:18:33,550] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 04:18:33,639] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240330T041830, end_date=20240330T041833
[2024-03-30 04:18:33,718] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 04:18:33,875] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-30 04:25:03,234] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 04:25:03,298] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 04:25:03,299] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:25:03,299] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 04:25:03,299] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:25:03,329] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-30 04:25:03,351] {standard_task_runner.py:52} INFO - Started process 4259 to run task
[2024-03-30 04:25:03,389] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '2188', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpplmhyu0m', '--error-file', '/tmp/tmp__jifutp']
[2024-03-30 04:25:03,423] {standard_task_runner.py:77} INFO - Job 2188: Subtask local_to_gcs_task
[2024-03-30 04:25:03,656] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 04:25:03,829] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 04:25:03,988] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-30 04:25:04,885] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2022-03.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2022_03.parquet.
[2024-03-30 04:25:04,886] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 04:25:04,933] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240330T042503, end_date=20240330T042504
[2024-03-30 04:25:05,038] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 04:25:05,134] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-30 04:39:12,937] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 04:39:13,009] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 04:39:13,010] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:39:13,010] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 04:39:13,010] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:39:13,057] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-30 04:39:13,084] {standard_task_runner.py:52} INFO - Started process 5306 to run task
[2024-03-30 04:39:13,107] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '2296', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp50tu4qq_', '--error-file', '/tmp/tmp__lrzy2v']
[2024-03-30 04:39:13,175] {standard_task_runner.py:77} INFO - Job 2296: Subtask local_to_gcs_task
[2024-03-30 04:39:13,594] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 04:39:13,923] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 04:39:14,131] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-30 04:39:16,236] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2022-03.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2022_03.parquet.
[2024-03-30 04:39:16,237] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 04:39:16,527] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240330T043912, end_date=20240330T043916
[2024-03-30 04:39:16,881] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 04:39:17,303] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-03-30 04:59:47,900] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 04:59:48,056] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 04:59:48,057] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:59:48,057] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 04:59:48,058] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:59:48,150] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-30 04:59:48,204] {standard_task_runner.py:52} INFO - Started process 6721 to run task
[2024-03-30 04:59:48,263] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '2433', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp6a2zom9l', '--error-file', '/tmp/tmpik59xmd0']
[2024-03-30 04:59:48,344] {standard_task_runner.py:77} INFO - Job 2433: Subtask local_to_gcs_task
[2024-03-30 04:59:48,861] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 04:59:49,181] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 04:59:49,323] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-30 04:59:50,967] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2022-03.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2022_03.parquet.
[2024-03-30 04:59:50,968] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 04:59:50,993] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240330T045947, end_date=20240330T045950
[2024-03-30 04:59:51,146] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 04:59:51,811] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-03-30 09:01:50,461] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 09:01:50,505] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 09:01:50,506] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 09:01:50,506] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 09:01:50,506] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 09:01:50,543] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-30 09:01:50,589] {standard_task_runner.py:52} INFO - Started process 18224 to run task
[2024-03-30 09:01:50,690] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '2543', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmptju7vti8', '--error-file', '/tmp/tmpnf4k6926']
[2024-03-30 09:01:50,721] {standard_task_runner.py:77} INFO - Job 2543: Subtask local_to_gcs_task
[2024-03-30 09:01:51,180] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 09:01:51,790] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 09:01:52,019] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-30 09:01:54,193] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2022-03.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2022_03.parquet.
[2024-03-30 09:01:54,194] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 09:01:54,400] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240330T090150, end_date=20240330T090154
[2024-03-30 09:01:54,655] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 09:01:54,875] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-03-30 11:15:32,395] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 11:15:32,619] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 11:15:32,619] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 11:15:32,619] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 11:15:32,619] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 11:15:33,058] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-30 11:15:33,072] {standard_task_runner.py:52} INFO - Started process 25427 to run task
[2024-03-30 11:15:33,340] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '2747', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpn85nrfxy', '--error-file', '/tmp/tmptylx7iok']
[2024-03-30 11:15:33,480] {standard_task_runner.py:77} INFO - Job 2747: Subtask local_to_gcs_task
[2024-03-30 11:15:34,869] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 11:15:35,314] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 11:15:35,919] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-30 11:16:12,950] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 65, in upload_to_gcs
    index=False,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/util/_decorators.py", line 199, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/core/frame.py", line 2372, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 276, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 101, in write
    table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
  File "pyarrow/table.pxi", line 1561, in pyarrow.lib.Table.from_pandas
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 607, in dataframe_to_arrays
    arrays[i] = maybe_fut.result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.6/concurrent/futures/thread.py", line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 581, in convert_column
    raise e
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 575, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
  File "pyarrow/array.pxi", line 302, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 83, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 122, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: ("Expected bytes, got a 'int' object", 'Conversion failed for column store_and_fwd_flag with type object')
[2024-03-30 11:16:12,975] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240330T111532, end_date=20240330T111612
[2024-03-30 11:16:12,993] {standard_task_runner.py:92} ERROR - Failed to execute job 2747 for task local_to_gcs_task
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 65, in upload_to_gcs
    index=False,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/util/_decorators.py", line 199, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/core/frame.py", line 2372, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 276, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 101, in write
    table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
  File "pyarrow/table.pxi", line 1561, in pyarrow.lib.Table.from_pandas
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 607, in dataframe_to_arrays
    arrays[i] = maybe_fut.result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.6/concurrent/futures/thread.py", line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 581, in convert_column
    raise e
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 575, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
  File "pyarrow/array.pxi", line 302, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 83, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 122, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: ("Expected bytes, got a 'int' object", 'Conversion failed for column store_and_fwd_flag with type object')
[2024-03-30 11:16:13,099] {local_task_job.py:154} INFO - Task exited with return code 1
[2024-03-30 11:16:13,142] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-03-30 11:34:15,904] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 11:34:16,039] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-03-30 11:34:16,039] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 11:34:16,039] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 11:34:16,039] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 11:34:16,117] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-03-30 11:34:16,126] {standard_task_runner.py:52} INFO - Started process 27019 to run task
[2024-03-30 11:34:16,184] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '2825', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpv6ylb702', '--error-file', '/tmp/tmp2n5ad9n2']
[2024-03-30 11:34:16,219] {standard_task_runner.py:77} INFO - Job 2825: Subtask local_to_gcs_task
[2024-03-30 11:34:16,825] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 11:34:17,359] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 11:34:17,435] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-03-30 11:40:32,588] {base_job.py:230} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2336, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 364, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 778, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 495, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 241, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 309, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 440, in __init__
    self.__connect(first_connect_check=True)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 656, in __connect
    connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 508, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.6/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/jobs/base_job.py", line 220, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2171, in merge
    _resolve_conflict_map=_resolve_conflict_map,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2244, in _merge
    merged = self.query(mapper.class_).get(key[1])
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 1018, in get
    return self._get_impl(ident, loading.load_on_pk_identity)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 1135, in _get_impl
    return db_load_fn(self, primary_key_identity)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/loading.py", line 286, in load_on_pk_identity
    return q.one()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3490, in one
    ret = self.one_or_none()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3459, in one_or_none
    ret = list(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3535, in __iter__
    return self._execute_and_instances(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3557, in _execute_and_instances
    querycontext, self._connection_from_session, close_with_result=True
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3572, in _get_bind_args
    mapper=self._bind_mapper(), clause=querycontext.statement, **kw
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3550, in _connection_from_session
    conn = self.session.connection(**kw)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1145, in connection
    execution_options=execution_options,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1151, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 433, in _connection_for_bind
    conn = bind._contextual_connect()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2302, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2340, in _wrap_pool_connect
    e, dialect, self
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1584, in _handle_dbapi_exception_noconnection
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2336, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 364, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 778, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 495, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 241, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 309, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 440, in __init__
    self.__connect(first_connect_check=True)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 656, in __connect
    connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 508, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.6/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.

(Background on this error at: http://sqlalche.me/e/13/e3q8)
[2024-03-30 11:40:32,748] {local_task_job.py:142} ERROR - Heartbeat time limit exceeded!
[2024-03-30 11:40:32,763] {process_utils.py:124} INFO - Sending Signals.SIGTERM to group 27019. PIDs of all processes in the group: [27019]
[2024-03-30 11:40:32,836] {process_utils.py:75} INFO - Sending the signal Signals.SIGTERM to group 27019
[2024-03-30 11:40:33,403] {taskinstance.py:1408} ERROR - Received SIGTERM. Terminating subprocesses.
[2024-03-30 11:40:33,436] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 64, in upload_to_gcs
    df = pd.read_parquet(raw_file_name)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 317, in read_parquet
    return impl.read(path, columns=columns, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 142, in read
    path, columns=columns, filesystem=fs, **kwargs
  File "pyarrow/array.pxi", line 757, in pyarrow.lib._PandasConvertible.to_pandas
  File "pyarrow/table.pxi", line 1748, in pyarrow.lib.Table._to_pandas
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 789, in table_to_blockmanager
    blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 1129, in _table_to_blocks
    list(extension_columns.keys()))
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1410, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2024-03-30 11:40:33,516] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240330T113415, end_date=20240330T114033
[2024-03-30 11:40:33,611] {standard_task_runner.py:92} ERROR - Failed to execute job 2825 for task local_to_gcs_task
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 64, in upload_to_gcs
    df = pd.read_parquet(raw_file_name)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 317, in read_parquet
    return impl.read(path, columns=columns, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 142, in read
    path, columns=columns, filesystem=fs, **kwargs
  File "pyarrow/array.pxi", line 757, in pyarrow.lib._PandasConvertible.to_pandas
  File "pyarrow/table.pxi", line 1748, in pyarrow.lib.Table._to_pandas
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 789, in table_to_blockmanager
    blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 1129, in _table_to_blocks
    list(extension_columns.keys()))
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1410, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2024-03-30 11:40:33,778] {process_utils.py:70} INFO - Process psutil.Process(pid=27019, status='terminated', exitcode=1, started='11:34:15') (27019) terminated with exit code 1
[2024-04-28 09:15:45,344] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-04-28 09:15:45,367] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-04-28 09:15:45,370] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-04-28 09:15:45,370] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-04-28 09:15:45,371] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-04-28 09:15:45,408] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-04-28 09:15:45,474] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '3011', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp39x20edd', '--error-file', '/tmp/tmpwik5boog']
[2024-04-28 09:15:45,520] {standard_task_runner.py:77} INFO - Job 3011: Subtask local_to_gcs_task
[2024-04-28 09:15:45,468] {standard_task_runner.py:52} INFO - Started process 1508 to run task
[2024-04-28 09:15:45,928] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host 6c6a4ca9ff8e
[2024-04-28 09:15:46,224] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-04-28 09:15:46,455] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-04-28 09:16:35,845] {local_task_job.py:154} INFO - Task exited with return code Negsignal.SIGKILL
[2024-04-28 09:16:36,237] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240428T091545, end_date=20240428T091636
[2024-04-28 09:16:36,553] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-30 02:14:04,654] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-04-30 02:14:04,826] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-04-30 02:14:04,838] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 02:14:04,839] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-04-30 02:14:04,841] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 02:14:05,489] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-04-30 02:14:05,556] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '3053', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpabasyt50', '--error-file', '/tmp/tmpihidvy60']
[2024-04-30 02:14:05,653] {standard_task_runner.py:77} INFO - Job 3053: Subtask local_to_gcs_task
[2024-04-30 02:14:05,542] {standard_task_runner.py:52} INFO - Started process 460 to run task
[2024-04-30 02:14:06,506] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host a1ae6a0259c5
[2024-04-30 02:14:07,191] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-04-30 02:14:07,499] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-04-30 02:14:57,960] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 75, in upload_to_gcs
    __clean_file(raw_file_name, clean_file_name)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 61, in __clean_file
    df.to_parquet(clean_file_name, index=False)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/util/_decorators.py", line 199, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/core/frame.py", line 2372, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 276, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 101, in write
    table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
  File "pyarrow/table.pxi", line 1561, in pyarrow.lib.Table.from_pandas
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 607, in dataframe_to_arrays
    arrays[i] = maybe_fut.result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.6/concurrent/futures/thread.py", line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 581, in convert_column
    raise e
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 575, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
  File "pyarrow/array.pxi", line 302, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 83, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 122, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: ("Expected bytes, got a 'int' object", 'Conversion failed for column store_and_fwd_flag with type object')
[2024-04-30 02:14:58,019] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240430T021404, end_date=20240430T021458
[2024-04-30 02:14:58,217] {standard_task_runner.py:92} ERROR - Failed to execute job 3053 for task local_to_gcs_task
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 75, in upload_to_gcs
    __clean_file(raw_file_name, clean_file_name)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 61, in __clean_file
    df.to_parquet(clean_file_name, index=False)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/util/_decorators.py", line 199, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/core/frame.py", line 2372, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 276, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 101, in write
    table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
  File "pyarrow/table.pxi", line 1561, in pyarrow.lib.Table.from_pandas
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 607, in dataframe_to_arrays
    arrays[i] = maybe_fut.result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.6/concurrent/futures/thread.py", line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 581, in convert_column
    raise e
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 575, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
  File "pyarrow/array.pxi", line 302, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 83, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 122, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: ("Expected bytes, got a 'int' object", 'Conversion failed for column store_and_fwd_flag with type object')
[2024-04-30 02:14:58,361] {local_task_job.py:154} INFO - Task exited with return code 1
[2024-04-30 02:14:58,415] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-30 02:58:26,827] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-04-30 02:58:26,880] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-04-30 02:58:26,880] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 02:58:26,880] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2024-04-30 02:58:26,880] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 02:58:27,158] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-04-30 02:58:27,167] {standard_task_runner.py:52} INFO - Started process 479 to run task
[2024-04-30 02:58:27,184] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '3116', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpx9mc_db2', '--error-file', '/tmp/tmptn7190ab']
[2024-04-30 02:58:27,193] {standard_task_runner.py:77} INFO - Job 3116: Subtask local_to_gcs_task
[2024-04-30 02:58:27,739] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host 02dc49e0e362
[2024-04-30 02:58:27,931] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-04-30 02:58:28,070] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-04-30 02:58:28,172] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 92, in upload_to_gcs
    blob.upload_from_filename(clean_file_name)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2720, in upload_from_filename
    with open(filename, "rb") as file_obj:
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/cleaned_data/yellowtaxi_tripdata_2022-03.parquet'
[2024-04-30 02:58:28,273] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240430T025826, end_date=20240430T025828
[2024-04-30 02:58:28,381] {standard_task_runner.py:92} ERROR - Failed to execute job 3116 for task local_to_gcs_task
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 92, in upload_to_gcs
    blob.upload_from_filename(clean_file_name)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2720, in upload_from_filename
    with open(filename, "rb") as file_obj:
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/cleaned_data/yellowtaxi_tripdata_2022-03.parquet'
[2024-04-30 02:58:28,475] {local_task_job.py:154} INFO - Task exited with return code 1
[2024-04-30 02:58:28,752] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-30 03:01:24,305] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-04-30 03:01:24,458] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-04-30 03:01:24,461] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 03:01:24,461] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2024-04-30 03:01:24,461] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 03:01:24,594] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-04-30 03:01:24,735] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '3152', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpovzemmbz', '--error-file', '/tmp/tmplhifs90o']
[2024-04-30 03:01:24,779] {standard_task_runner.py:77} INFO - Job 3152: Subtask local_to_gcs_task
[2024-04-30 03:01:24,667] {standard_task_runner.py:52} INFO - Started process 762 to run task
[2024-04-30 03:01:25,257] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host 02dc49e0e362
[2024-04-30 03:01:25,400] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-04-30 03:01:25,539] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-04-30 03:01:26,704] {python.py:175} INFO - Done. Returned value was: None
[2024-04-30 03:01:26,741] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240430T030124, end_date=20240430T030126
[2024-04-30 03:01:26,830] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-04-30 03:01:26,970] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-04-30 04:30:20,196] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-04-30 04:30:20,333] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [queued]>
[2024-04-30 04:30:20,333] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 04:30:20,333] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2024-04-30 04:30:20,333] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 04:30:20,474] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2022-03-02 06:00:00+00:00
[2024-04-30 04:30:20,696] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2022-03-02T06:00:00+00:00', '--job-id', '3262', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp8om4_0vw', '--error-file', '/tmp/tmpza2iirtx']
[2024-04-30 04:30:20,733] {standard_task_runner.py:77} INFO - Job 3262: Subtask local_to_gcs_task
[2024-04-30 04:30:20,622] {standard_task_runner.py:52} INFO - Started process 5340 to run task
[2024-04-30 04:30:21,473] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2022-03-02T06:00:00+00:00 [running]> on host 02dc49e0e362
[2024-04-30 04:30:22,063] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-04-30 04:30:22,358] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2022-03-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2022-03-02T06:00:00+00:00
[2024-04-30 04:31:05,394] {local_task_job.py:154} INFO - Task exited with return code Negsignal.SIGKILL
[2024-04-30 04:31:05,698] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20220302T060000, start_date=20240430T043020, end_date=20240430T043105
[2024-04-30 04:31:06,170] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
