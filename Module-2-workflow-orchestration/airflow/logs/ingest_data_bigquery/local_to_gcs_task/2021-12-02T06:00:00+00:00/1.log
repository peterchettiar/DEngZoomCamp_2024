[2024-03-29 07:24:44,297] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-29 07:24:44,402] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-29 07:24:44,403] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 07:24:44,404] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-29 07:24:44,408] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 07:24:44,497] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-29 07:24:44,572] {standard_task_runner.py:52} INFO - Started process 591 to run task
[2024-03-29 07:24:44,590] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '1462', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp49r2v_4y', '--error-file', '/tmp/tmpgqjsyht9']
[2024-03-29 07:24:44,661] {standard_task_runner.py:77} INFO - Job 1462: Subtask local_to_gcs_task
[2024-03-29 07:24:45,209] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host b8c510ab3cfc
[2024-03-29 07:24:45,806] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-29 07:24:46,333] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-29 07:24:47,495] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2021-12.parquet uploaded to yellowtaxi_tripdata/2021-12.parquet.
[2024-03-29 07:24:47,495] {python.py:175} INFO - Done. Returned value was: None
[2024-03-29 07:24:47,575] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240329T072444, end_date=20240329T072447
[2024-03-29 07:24:48,090] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-29 07:24:48,571] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-29 07:31:02,878] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-29 07:31:02,945] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-29 07:31:02,953] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 07:31:02,954] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-29 07:31:02,954] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 07:31:03,036] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-29 07:31:03,142] {standard_task_runner.py:52} INFO - Started process 1094 to run task
[2024-03-29 07:31:03,213] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '1523', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpdezecg2i', '--error-file', '/tmp/tmpbcs8jlx_']
[2024-03-29 07:31:03,266] {standard_task_runner.py:77} INFO - Job 1523: Subtask local_to_gcs_task
[2024-03-29 07:31:03,627] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host b8c510ab3cfc
[2024-03-29 07:31:03,743] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-29 07:31:03,838] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-29 07:31:04,931] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2021-12.parquet uploaded to yellowtaxi_tripdata/2021-12.parquet.
[2024-03-29 07:31:04,932] {python.py:175} INFO - Done. Returned value was: None
[2024-03-29 07:31:04,961] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240329T073102, end_date=20240329T073104
[2024-03-29 07:31:05,148] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-29 07:31:05,277] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-29 07:37:49,699] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-29 07:37:49,766] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-29 07:37:49,767] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 07:37:49,768] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-29 07:37:49,770] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 07:37:49,847] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-29 07:37:49,881] {standard_task_runner.py:52} INFO - Started process 1629 to run task
[2024-03-29 07:37:49,918] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '1589', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp4mzktrkm', '--error-file', '/tmp/tmpc8c773gh']
[2024-03-29 07:37:49,964] {standard_task_runner.py:77} INFO - Job 1589: Subtask local_to_gcs_task
[2024-03-29 07:37:50,187] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host b8c510ab3cfc
[2024-03-29 07:37:50,346] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-29 07:37:50,456] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-29 07:37:51,660] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2021-12.parquet uploaded to yellowtaxi_tripdata/2021-12.parquet.
[2024-03-29 07:37:51,661] {python.py:175} INFO - Done. Returned value was: None
[2024-03-29 07:37:51,686] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240329T073749, end_date=20240329T073751
[2024-03-29 07:37:51,800] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-29 07:37:51,969] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-29 08:08:45,977] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-29 08:08:46,020] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-29 08:08:46,119] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 08:08:46,129] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-29 08:08:46,131] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 08:08:46,622] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-29 08:08:46,814] {standard_task_runner.py:52} INFO - Started process 309 to run task
[2024-03-29 08:08:46,863] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '1654', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpyplf56ig', '--error-file', '/tmp/tmpptv3ol4x']
[2024-03-29 08:08:46,892] {standard_task_runner.py:77} INFO - Job 1654: Subtask local_to_gcs_task
[2024-03-29 08:08:47,477] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host b5e5a5778b83
[2024-03-29 08:08:47,710] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-29 08:08:47,847] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-29 08:08:48,903] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2594, in upload_from_file
    retry=retry,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2412, in _do_upload
    retry=retry,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2242, in _do_resumable_upload
    response = upload.transmit_next_chunk(transport, timeout=timeout)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py", line 516, in transmit_next_chunk
    retriable_request, self._get_status_code, self._retry_strategy
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/_request_helpers.py", line 170, in wait_and_retry
    raise error
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/_request_helpers.py", line 147, in wait_and_retry
    response = func()
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py", line 511, in retriable_request
    self._process_response(result, len(payload))
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/_upload.py", line 675, in _process_response
    callback=self._make_invalid,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/_helpers.py", line 104, in require_status_code
    *status_codes
google.resumable_media.common.InvalidResponse: ('Request failed with status code', 429, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PERMANENT_REDIRECT: 308>)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 43, in upload_to_gcs
    blob.upload_from_filename(source_file_name)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2735, in upload_from_filename
    retry=retry,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2598, in upload_from_file
    _raise_from_invalid_response(exc)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 4466, in _raise_from_invalid_response
    raise exceptions.from_http_status(response.status_code, message, response=response)
google.api_core.exceptions.TooManyRequests: 429 PUT https://storage.googleapis.com/upload/storage/v1/b/test_bucket_415106/o?uploadType=resumable&upload_id=ABPtcPrxC8h3HGQsBR9nDnYsjQ4yCCA7Br0ijZvvb9gf5OybhN_CXRxsG2BMWql9yHj346m5kAAIW83EdwOM8DVMBpaHPYq5J9ykwfBC1TawpIfS3RU: {
  "error": {
    "code": 429,
    "message": "The object test_bucket_415106/yellowtaxi_tripdata/{TABLE_NAME_TEMPLATE}.parquet exceeded the rate limit for object mutation operations (create, update, and delete). Please reduce your request rate. See https://cloud.google.com/storage/docs/gcs429.",
    "errors": [
      {
        "message": "The object test_bucket_415106/yellowtaxi_tripdata/{TABLE_NAME_TEMPLATE}.parquet exceeded the rate limit for object mutation operations (create, update, and delete). Please reduce your request rate. See https://cloud.google.com/storage/docs/gcs429.",
        "domain": "usageLimits",
        "reason": "rateLimitExceeded"
      }
    ]
  }
}
: ('Request failed with status code', 429, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PERMANENT_REDIRECT: 308>)
[2024-03-29 08:08:49,434] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240329T080845, end_date=20240329T080849
[2024-03-29 08:08:49,524] {standard_task_runner.py:92} ERROR - Failed to execute job 1654 for task local_to_gcs_task
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2594, in upload_from_file
    retry=retry,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2412, in _do_upload
    retry=retry,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2242, in _do_resumable_upload
    response = upload.transmit_next_chunk(transport, timeout=timeout)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py", line 516, in transmit_next_chunk
    retriable_request, self._get_status_code, self._retry_strategy
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/_request_helpers.py", line 170, in wait_and_retry
    raise error
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/_request_helpers.py", line 147, in wait_and_retry
    response = func()
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/requests/upload.py", line 511, in retriable_request
    self._process_response(result, len(payload))
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/_upload.py", line 675, in _process_response
    callback=self._make_invalid,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/resumable_media/_helpers.py", line 104, in require_status_code
    *status_codes
google.resumable_media.common.InvalidResponse: ('Request failed with status code', 429, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PERMANENT_REDIRECT: 308>)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 43, in upload_to_gcs
    blob.upload_from_filename(source_file_name)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2735, in upload_from_filename
    retry=retry,
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2598, in upload_from_file
    _raise_from_invalid_response(exc)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 4466, in _raise_from_invalid_response
    raise exceptions.from_http_status(response.status_code, message, response=response)
google.api_core.exceptions.TooManyRequests: 429 PUT https://storage.googleapis.com/upload/storage/v1/b/test_bucket_415106/o?uploadType=resumable&upload_id=ABPtcPrxC8h3HGQsBR9nDnYsjQ4yCCA7Br0ijZvvb9gf5OybhN_CXRxsG2BMWql9yHj346m5kAAIW83EdwOM8DVMBpaHPYq5J9ykwfBC1TawpIfS3RU: {
  "error": {
    "code": 429,
    "message": "The object test_bucket_415106/yellowtaxi_tripdata/{TABLE_NAME_TEMPLATE}.parquet exceeded the rate limit for object mutation operations (create, update, and delete). Please reduce your request rate. See https://cloud.google.com/storage/docs/gcs429.",
    "errors": [
      {
        "message": "The object test_bucket_415106/yellowtaxi_tripdata/{TABLE_NAME_TEMPLATE}.parquet exceeded the rate limit for object mutation operations (create, update, and delete). Please reduce your request rate. See https://cloud.google.com/storage/docs/gcs429.",
        "domain": "usageLimits",
        "reason": "rateLimitExceeded"
      }
    ]
  }
}
: ('Request failed with status code', 429, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PERMANENT_REDIRECT: 308>)
[2024-03-29 08:08:49,603] {local_task_job.py:154} INFO - Task exited with return code 1
[2024-03-29 08:08:50,080] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-03-29 08:11:18,604] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-29 08:11:18,681] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-29 08:11:18,698] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 08:11:18,698] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-29 08:11:18,698] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-29 08:11:18,742] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-29 08:11:18,783] {standard_task_runner.py:52} INFO - Started process 597 to run task
[2024-03-29 08:11:18,843] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '1694', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpe860kggp', '--error-file', '/tmp/tmpi69boec7']
[2024-03-29 08:11:18,933] {standard_task_runner.py:77} INFO - Job 1694: Subtask local_to_gcs_task
[2024-03-29 08:11:19,135] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host b5e5a5778b83
[2024-03-29 08:11:19,275] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-29 08:11:19,366] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-29 08:11:20,612] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2021-12.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2021_12.parquet.
[2024-03-29 08:11:20,613] {python.py:175} INFO - Done. Returned value was: None
[2024-03-29 08:11:20,680] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240329T081118, end_date=20240329T081120
[2024-03-29 08:11:20,856] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-29 08:11:21,036] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-30 03:30:21,390] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 03:30:21,491] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 03:30:21,492] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 03:30:21,492] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 03:30:21,492] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 03:30:21,631] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-30 03:30:21,658] {standard_task_runner.py:52} INFO - Started process 538 to run task
[2024-03-30 03:30:21,741] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '1841', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp9msbaf4w', '--error-file', '/tmp/tmp3m6jz298']
[2024-03-30 03:30:21,816] {standard_task_runner.py:77} INFO - Job 1841: Subtask local_to_gcs_task
[2024-03-30 03:30:22,429] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 03:30:22,788] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 03:30:23,073] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-30 03:30:25,400] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2021-12.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2021_12.parquet.
[2024-03-30 03:30:25,401] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 03:30:25,443] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240330T033021, end_date=20240330T033025
[2024-03-30 03:30:25,623] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 03:30:26,078] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-03-30 03:42:15,885] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 03:42:15,915] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 03:42:15,915] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 03:42:15,915] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 03:42:15,915] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 03:42:15,933] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-30 03:42:15,951] {standard_task_runner.py:52} INFO - Started process 1376 to run task
[2024-03-30 03:42:16,025] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '1921', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpqzu0vb0j', '--error-file', '/tmp/tmp65rfj0b8']
[2024-03-30 03:42:16,030] {standard_task_runner.py:77} INFO - Job 1921: Subtask local_to_gcs_task
[2024-03-30 03:42:16,306] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 03:42:16,529] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 03:42:16,723] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-30 03:42:18,622] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2021-12.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2021_12.parquet.
[2024-03-30 03:42:18,626] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 03:42:19,089] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240330T034215, end_date=20240330T034219
[2024-03-30 03:42:19,296] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 03:42:19,564] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-30 04:18:30,128] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 04:18:30,248] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 04:18:30,248] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:18:30,248] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 04:18:30,248] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:18:30,452] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-30 04:18:30,826] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '2102', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp7v4v_nh_', '--error-file', '/tmp/tmpr694omqy']
[2024-03-30 04:18:30,841] {standard_task_runner.py:77} INFO - Job 2102: Subtask local_to_gcs_task
[2024-03-30 04:18:30,823] {standard_task_runner.py:52} INFO - Started process 3629 to run task
[2024-03-30 04:18:31,367] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 04:18:31,745] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 04:18:31,899] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-30 04:18:33,543] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2021-12.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2021_12.parquet.
[2024-03-30 04:18:33,547] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 04:18:33,646] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240330T041830, end_date=20240330T041833
[2024-03-30 04:18:33,775] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 04:18:33,896] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-30 04:25:01,350] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 04:25:01,375] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 04:25:01,375] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:25:01,375] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 04:25:01,375] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:25:01,414] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-30 04:25:01,468] {standard_task_runner.py:52} INFO - Started process 4251 to run task
[2024-03-30 04:25:01,513] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '2184', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp6ixd8a2m', '--error-file', '/tmp/tmpk75xy8xb']
[2024-03-30 04:25:01,615] {standard_task_runner.py:77} INFO - Job 2184: Subtask local_to_gcs_task
[2024-03-30 04:25:01,991] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 04:25:02,217] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 04:25:02,330] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-30 04:25:03,560] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2021-12.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2021_12.parquet.
[2024-03-30 04:25:03,561] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 04:25:03,615] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240330T042501, end_date=20240330T042503
[2024-03-30 04:25:03,716] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 04:25:03,848] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-30 04:39:12,367] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 04:39:12,470] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 04:39:12,471] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:39:12,471] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 04:39:12,471] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:39:12,583] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-30 04:39:12,605] {standard_task_runner.py:52} INFO - Started process 5304 to run task
[2024-03-30 04:39:12,653] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '2294', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpjtxhcrqi', '--error-file', '/tmp/tmpeu518slw']
[2024-03-30 04:39:12,703] {standard_task_runner.py:77} INFO - Job 2294: Subtask local_to_gcs_task
[2024-03-30 04:39:12,925] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 04:39:13,147] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 04:39:13,295] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-30 04:39:15,151] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2021-12.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2021_12.parquet.
[2024-03-30 04:39:15,155] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 04:39:15,311] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240330T043912, end_date=20240330T043915
[2024-03-30 04:39:15,595] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 04:39:15,699] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-30 04:59:41,623] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 04:59:41,747] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 04:59:41,747] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:59:41,747] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 04:59:41,747] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 04:59:41,914] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-30 04:59:41,973] {standard_task_runner.py:52} INFO - Started process 6696 to run task
[2024-03-30 04:59:42,041] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '2426', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpqxns32zi', '--error-file', '/tmp/tmpcn6zq60g']
[2024-03-30 04:59:42,114] {standard_task_runner.py:77} INFO - Job 2426: Subtask local_to_gcs_task
[2024-03-30 04:59:42,448] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 04:59:42,646] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 04:59:42,763] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-30 04:59:44,911] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2021-12.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2021_12.parquet.
[2024-03-30 04:59:44,911] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 04:59:45,419] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240330T045941, end_date=20240330T045945
[2024-03-30 04:59:45,760] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 04:59:45,953] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-03-30 09:01:51,011] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 09:01:51,183] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 09:01:51,183] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 09:01:51,183] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 09:01:51,183] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 09:01:51,354] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-30 09:01:51,441] {standard_task_runner.py:52} INFO - Started process 18227 to run task
[2024-03-30 09:01:51,570] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '2546', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp26lux6g5', '--error-file', '/tmp/tmpum1xqnnu']
[2024-03-30 09:01:51,631] {standard_task_runner.py:77} INFO - Job 2546: Subtask local_to_gcs_task
[2024-03-30 09:01:52,042] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 09:01:52,450] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 09:01:52,567] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-30 09:01:53,841] {logging_mixin.py:109} INFO - File /opt/***/yellowtaxi_tripdata_2021-12.parquet uploaded to yellowtaxi_tripdata/yellow_taxi_2021_12.parquet.
[2024-03-30 09:01:53,841] {python.py:175} INFO - Done. Returned value was: None
[2024-03-30 09:01:53,954] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240330T090151, end_date=20240330T090153
[2024-03-30 09:01:54,118] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-03-30 09:01:54,446] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-03-30 11:04:31,499] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 11:04:31,624] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 11:04:31,625] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 11:04:31,626] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 11:04:31,627] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 11:04:31,886] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-30 11:04:32,011] {standard_task_runner.py:52} INFO - Started process 24497 to run task
[2024-03-30 11:04:32,112] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '2697', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp_x8s57e7', '--error-file', '/tmp/tmpcqovcs7y']
[2024-03-30 11:04:32,166] {standard_task_runner.py:77} INFO - Job 2697: Subtask local_to_gcs_task
[2024-03-30 11:04:44,602] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 11:04:44,930] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 11:04:45,128] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-30 11:05:22,369] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 59, in upload_to_gcs
    index=False,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/util/_decorators.py", line 199, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/core/frame.py", line 2372, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 276, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 101, in write
    table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
  File "pyarrow/table.pxi", line 1561, in pyarrow.lib.Table.from_pandas
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 607, in dataframe_to_arrays
    arrays[i] = maybe_fut.result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.6/concurrent/futures/thread.py", line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 581, in convert_column
    raise e
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 575, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
  File "pyarrow/array.pxi", line 302, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 83, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 122, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: ("Expected bytes, got a 'int' object", 'Conversion failed for column store_and_fwd_flag with type object')
[2024-03-30 11:05:22,419] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240330T110431, end_date=20240330T110522
[2024-03-30 11:05:22,477] {standard_task_runner.py:92} ERROR - Failed to execute job 2697 for task local_to_gcs_task
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 59, in upload_to_gcs
    index=False,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/util/_decorators.py", line 199, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/core/frame.py", line 2372, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 276, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 101, in write
    table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
  File "pyarrow/table.pxi", line 1561, in pyarrow.lib.Table.from_pandas
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 607, in dataframe_to_arrays
    arrays[i] = maybe_fut.result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.6/concurrent/futures/thread.py", line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 581, in convert_column
    raise e
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 575, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
  File "pyarrow/array.pxi", line 302, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 83, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 122, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: ("Expected bytes, got a 'int' object", 'Conversion failed for column store_and_fwd_flag with type object')
[2024-03-30 11:05:22,679] {local_task_job.py:154} INFO - Task exited with return code 1
[2024-03-30 11:05:22,769] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-03-30 11:15:31,440] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 11:15:31,629] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 11:15:31,629] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 11:15:31,633] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 11:15:31,633] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 11:15:31,700] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-30 11:15:31,714] {standard_task_runner.py:52} INFO - Started process 25414 to run task
[2024-03-30 11:15:31,781] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '2745', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmphn75ppgt', '--error-file', '/tmp/tmpo1k0hq0t']
[2024-03-30 11:15:31,820] {standard_task_runner.py:77} INFO - Job 2745: Subtask local_to_gcs_task
[2024-03-30 11:15:32,254] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 11:15:32,441] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 11:15:32,918] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-30 11:16:08,951] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 65, in upload_to_gcs
    index=False,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/util/_decorators.py", line 199, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/core/frame.py", line 2372, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 276, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 101, in write
    table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
  File "pyarrow/table.pxi", line 1561, in pyarrow.lib.Table.from_pandas
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 607, in dataframe_to_arrays
    arrays[i] = maybe_fut.result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.6/concurrent/futures/thread.py", line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 581, in convert_column
    raise e
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 575, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
  File "pyarrow/array.pxi", line 302, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 83, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 122, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: ("Expected bytes, got a 'int' object", 'Conversion failed for column store_and_fwd_flag with type object')
[2024-03-30 11:16:08,992] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240330T111531, end_date=20240330T111608
[2024-03-30 11:16:09,048] {standard_task_runner.py:92} ERROR - Failed to execute job 2745 for task local_to_gcs_task
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 65, in upload_to_gcs
    index=False,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/util/_decorators.py", line 199, in wrapper
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/core/frame.py", line 2372, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 276, in to_parquet
    **kwargs,
  File "/home/airflow/.local/lib/python3.6/site-packages/pandas/io/parquet.py", line 101, in write
    table = self.api.Table.from_pandas(df, **from_pandas_kwargs)
  File "pyarrow/table.pxi", line 1561, in pyarrow.lib.Table.from_pandas
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 607, in dataframe_to_arrays
    arrays[i] = maybe_fut.result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 425, in result
    return self.__get_result()
  File "/usr/local/lib/python3.6/concurrent/futures/_base.py", line 384, in __get_result
    raise self._exception
  File "/usr/local/lib/python3.6/concurrent/futures/thread.py", line 56, in run
    result = self.fn(*self.args, **self.kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 581, in convert_column
    raise e
  File "/home/airflow/.local/lib/python3.6/site-packages/pyarrow/pandas_compat.py", line 575, in convert_column
    result = pa.array(col, type=type_, from_pandas=True, safe=safe)
  File "pyarrow/array.pxi", line 302, in pyarrow.lib.array
  File "pyarrow/array.pxi", line 83, in pyarrow.lib._ndarray_to_array
  File "pyarrow/error.pxi", line 122, in pyarrow.lib.check_status
pyarrow.lib.ArrowTypeError: ("Expected bytes, got a 'int' object", 'Conversion failed for column store_and_fwd_flag with type object')
[2024-03-30 11:16:09,242] {local_task_job.py:154} INFO - Task exited with return code 1
[2024-03-30 11:16:09,778] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-03-30 11:34:12,823] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 11:34:12,878] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-03-30 11:34:12,878] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 11:34:12,879] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-03-30 11:34:12,879] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-03-30 11:34:13,020] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-03-30 11:34:13,030] {standard_task_runner.py:52} INFO - Started process 26987 to run task
[2024-03-30 11:34:13,055] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '2821', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpirzdah6r', '--error-file', '/tmp/tmp2fzsccct']
[2024-03-30 11:34:13,067] {standard_task_runner.py:77} INFO - Job 2821: Subtask local_to_gcs_task
[2024-03-30 11:34:13,858] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 98c5b3fde4c4
[2024-03-30 11:34:14,423] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-03-30 11:34:14,642] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-03-30 11:40:32,592] {base_job.py:230} ERROR - LocalTaskJob heartbeat got an exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2336, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 364, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 778, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 495, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 241, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 309, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 440, in __init__
    self.__connect(first_connect_check=True)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 656, in __connect
    connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 508, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.6/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
psycopg2.OperationalError: connection to server at "postgres" (172.18.0.3), port 5432 failed: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/jobs/base_job.py", line 220, in heartbeat
    session.merge(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2171, in merge
    _resolve_conflict_map=_resolve_conflict_map,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 2244, in _merge
    merged = self.query(mapper.class_).get(key[1])
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 1018, in get
    return self._get_impl(ident, loading.load_on_pk_identity)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 1135, in _get_impl
    return db_load_fn(self, primary_key_identity)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/loading.py", line 286, in load_on_pk_identity
    return q.one()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3490, in one
    ret = self.one_or_none()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3459, in one_or_none
    ret = list(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3535, in __iter__
    return self._execute_and_instances(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3557, in _execute_and_instances
    querycontext, self._connection_from_session, close_with_result=True
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3572, in _get_bind_args
    mapper=self._bind_mapper(), clause=querycontext.statement, **kw
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/query.py", line 3550, in _connection_from_session
    conn = self.session.connection(**kw)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1145, in connection
    execution_options=execution_options,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 1151, in _connection_for_bind
    engine, execution_options
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/orm/session.py", line 433, in _connection_for_bind
    conn = bind._contextual_connect()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2302, in _contextual_connect
    self._wrap_pool_connect(self.pool.connect, None),
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2340, in _wrap_pool_connect
    e, dialect, self
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 1584, in _handle_dbapi_exception_noconnection
    sqlalchemy_exception, with_traceback=exc_info[2], from_=e
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/base.py", line 2336, in _wrap_pool_connect
    return fn()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 364, in connect
    return _ConnectionFairy._checkout(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 778, in _checkout
    fairy = _ConnectionRecord.checkout(pool)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 495, in checkout
    rec = pool._do_get()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/impl.py", line 241, in _do_get
    return self._create_connection()
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 309, in _create_connection
    return _ConnectionRecord(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 440, in __init__
    self.__connect(first_connect_check=True)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 661, in __connect
    pool.logger.debug("Error on connect(): %s", e)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/langhelpers.py", line 70, in __exit__
    with_traceback=exc_tb,
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/util/compat.py", line 182, in raise_
    raise exception
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/pool/base.py", line 656, in __connect
    connection = pool._invoke_creator(self)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/strategies.py", line 114, in connect
    return dialect.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.6/site-packages/sqlalchemy/engine/default.py", line 508, in connect
    return self.dbapi.connect(*cargs, **cparams)
  File "/home/airflow/.local/lib/python3.6/site-packages/psycopg2/__init__.py", line 122, in connect
    conn = _connect(dsn, connection_factory=connection_factory, **kwasync)
sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) connection to server at "postgres" (172.18.0.3), port 5432 failed: server closed the connection unexpectedly
	This probably means the server terminated abnormally
	before or while processing the request.

(Background on this error at: http://sqlalche.me/e/13/e3q8)
[2024-03-30 11:40:32,751] {local_task_job.py:142} ERROR - Heartbeat time limit exceeded!
[2024-03-30 11:40:32,765] {standard_task_runner.py:135} ERROR - Job 2821 was killed before it finished (likely due to running out of memory)
[2024-04-28 09:15:46,342] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-04-28 09:15:46,425] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-04-28 09:15:46,426] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-04-28 09:15:46,427] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-04-28 09:15:46,429] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-04-28 09:15:46,472] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-04-28 09:15:46,494] {standard_task_runner.py:52} INFO - Started process 1520 to run task
[2024-04-28 09:15:46,516] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '3013', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp3fsun4im', '--error-file', '/tmp/tmp7zfyy7ee']
[2024-04-28 09:15:46,540] {standard_task_runner.py:77} INFO - Job 3013: Subtask local_to_gcs_task
[2024-04-28 09:15:46,718] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 6c6a4ca9ff8e
[2024-04-28 09:15:46,850] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-04-28 09:15:46,964] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-04-28 09:16:35,855] {local_task_job.py:154} INFO - Task exited with return code Negsignal.SIGKILL
[2024-04-28 09:16:36,177] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240428T091546, end_date=20240428T091636
[2024-04-28 09:16:36,427] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-30 02:14:02,389] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-04-30 02:14:02,572] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-04-30 02:14:02,572] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 02:14:02,572] {taskinstance.py:1239} INFO - Starting attempt 1 of 4
[2024-04-30 02:14:02,573] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 02:14:02,719] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-04-30 02:14:02,794] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '3046', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmp8ls3lzlc', '--error-file', '/tmp/tmpk_oz2m8v']
[2024-04-30 02:14:02,842] {standard_task_runner.py:77} INFO - Job 3046: Subtask local_to_gcs_task
[2024-04-30 02:14:02,759] {standard_task_runner.py:52} INFO - Started process 423 to run task
[2024-04-30 02:14:03,495] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host a1ae6a0259c5
[2024-04-30 02:14:04,221] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-04-30 02:14:04,419] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-04-30 02:14:53,099] {local_task_job.py:154} INFO - Task exited with return code Negsignal.SIGKILL
[2024-04-30 02:14:53,463] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240430T021402, end_date=20240430T021453
[2024-04-30 02:14:54,333] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-30 02:58:27,608] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-04-30 02:58:27,689] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-04-30 02:58:27,689] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 02:58:27,689] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2024-04-30 02:58:27,689] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 02:58:27,770] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-04-30 02:58:27,799] {standard_task_runner.py:52} INFO - Started process 486 to run task
[2024-04-30 02:58:27,859] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '3121', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmprav2rrqa', '--error-file', '/tmp/tmp9k7tl7l3']
[2024-04-30 02:58:27,889] {standard_task_runner.py:77} INFO - Job 3121: Subtask local_to_gcs_task
[2024-04-30 02:58:28,313] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 02dc49e0e362
[2024-04-30 02:58:28,569] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-04-30 02:58:28,795] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-04-30 02:58:28,904] {taskinstance.py:1700} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 92, in upload_to_gcs
    blob.upload_from_filename(clean_file_name)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2720, in upload_from_filename
    with open(filename, "rb") as file_obj:
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/cleaned_data/yellowtaxi_tripdata_2021-12.parquet'
[2024-04-30 02:58:28,976] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240430T025827, end_date=20240430T025828
[2024-04-30 02:58:29,011] {standard_task_runner.py:92} ERROR - Failed to execute job 3121 for task local_to_gcs_task
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/task/task_runner/standard_task_runner.py", line 85, in _start_by_fork
    args.func(args, dag=self.dag)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/cli_parser.py", line 48, in command
    return func(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/cli.py", line 92, in wrapper
    return f(*args, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 298, in task_run
    _run_task_by_selected_method(args, dag, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 107, in _run_task_by_selected_method
    _run_raw_task(args, ti)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/cli/commands/task_command.py", line 184, in _run_raw_task
    error_file=args.error_file,
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/utils/session.py", line 70, in wrapper
    return func(*args, session=session, **kwargs)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1329, in _run_raw_task
    self._execute_task_with_callbacks(context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1455, in _execute_task_with_callbacks
    result = self._execute_task(context, self.task)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/models/taskinstance.py", line 1511, in _execute_task
    result = execute_callable(context=context)
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 174, in execute
    return_value = self.execute_callable()
  File "/home/airflow/.local/lib/python3.6/site-packages/airflow/operators/python.py", line 185, in execute_callable
    return self.python_callable(*self.op_args, **self.op_kwargs)
  File "/opt/airflow/dags/data_ingestion_gcs_dag.py", line 92, in upload_to_gcs
    blob.upload_from_filename(clean_file_name)
  File "/home/airflow/.local/lib/python3.6/site-packages/google/cloud/storage/blob.py", line 2720, in upload_from_filename
    with open(filename, "rb") as file_obj:
FileNotFoundError: [Errno 2] No such file or directory: '/opt/airflow/cleaned_data/yellowtaxi_tripdata_2021-12.parquet'
[2024-04-30 02:58:29,077] {local_task_job.py:154} INFO - Task exited with return code 1
[2024-04-30 02:58:29,134] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-04-30 03:01:24,294] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-04-30 03:01:24,481] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-04-30 03:01:24,481] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 03:01:24,481] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2024-04-30 03:01:24,481] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 03:01:24,647] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-04-30 03:01:24,739] {standard_task_runner.py:52} INFO - Started process 763 to run task
[2024-04-30 03:01:24,733] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '3154', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpzwm05ips', '--error-file', '/tmp/tmpznqhts4_']
[2024-04-30 03:01:24,855] {standard_task_runner.py:77} INFO - Job 3154: Subtask local_to_gcs_task
[2024-04-30 03:01:25,364] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 02dc49e0e362
[2024-04-30 03:01:25,690] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-04-30 03:01:25,802] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-04-30 03:01:27,016] {python.py:175} INFO - Done. Returned value was: None
[2024-04-30 03:01:27,076] {taskinstance.py:1277} INFO - Marking task as SUCCESS. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240430T030124, end_date=20240430T030127
[2024-04-30 03:01:27,255] {local_task_job.py:154} INFO - Task exited with return code 0
[2024-04-30 03:01:27,335] {local_task_job.py:264} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2024-04-30 04:30:24,215] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-04-30 04:30:24,427] {taskinstance.py:1032} INFO - Dependencies all met for <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [queued]>
[2024-04-30 04:30:24,468] {taskinstance.py:1238} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 04:30:24,479] {taskinstance.py:1239} INFO - Starting attempt 1 of 2
[2024-04-30 04:30:24,479] {taskinstance.py:1240} INFO - 
--------------------------------------------------------------------------------
[2024-04-30 04:30:24,877] {taskinstance.py:1259} INFO - Executing <Task(PythonOperator): local_to_gcs_task> on 2021-12-02 06:00:00+00:00
[2024-04-30 04:30:25,088] {standard_task_runner.py:52} INFO - Started process 5361 to run task
[2024-04-30 04:30:25,213] {standard_task_runner.py:76} INFO - Running: ['***', 'tasks', 'run', 'ingest_data_bigquery', 'local_to_gcs_task', 'scheduled__2021-12-02T06:00:00+00:00', '--job-id', '3265', '--raw', '--subdir', 'DAGS_FOLDER/data_ingestion_gcs_dag.py', '--cfg-path', '/tmp/tmpty7_lz1y', '--error-file', '/tmp/tmp2388vuav']
[2024-04-30 04:30:25,518] {standard_task_runner.py:77} INFO - Job 3265: Subtask local_to_gcs_task
[2024-04-30 04:30:30,516] {logging_mixin.py:109} INFO - Running <TaskInstance: ingest_data_bigquery.local_to_gcs_task scheduled__2021-12-02T06:00:00+00:00 [running]> on host 02dc49e0e362
[2024-04-30 04:30:39,646] {warnings.py:99} WARNING - /home/***/.local/lib/python3.6/site-packages/***/utils/context.py:152: AirflowContextDeprecationWarning: Accessing 'execution_date' from the template is deprecated and will be removed in a future version. Please use 'data_interval_start' or 'logical_date' instead.
  warnings.warn(_create_deprecation_warning(key, self._deprecation_replacements[key]))

[2024-04-30 04:30:57,026] {taskinstance.py:1426} INFO - Exporting the following env vars:
AIRFLOW_CTX_DAG_OWNER=***
AIRFLOW_CTX_DAG_ID=ingest_data_bigquery
AIRFLOW_CTX_TASK_ID=local_to_gcs_task
AIRFLOW_CTX_EXECUTION_DATE=2021-12-02T06:00:00+00:00
AIRFLOW_CTX_DAG_RUN_ID=scheduled__2021-12-02T06:00:00+00:00
[2024-04-30 04:33:11,342] {local_task_job.py:154} INFO - Task exited with return code Negsignal.SIGKILL
[2024-04-30 04:33:11,925] {taskinstance.py:1277} INFO - Marking task as UP_FOR_RETRY. dag_id=ingest_data_bigquery, task_id=local_to_gcs_task, execution_date=20211202T060000, start_date=20240430T043024, end_date=20240430T043311
[2024-04-30 04:33:12,691] {local_task_job.py:264} INFO - 0 downstream tasks scheduled from follow-on schedule check
